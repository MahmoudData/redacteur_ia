import streamlit as st
import tempfile
import os
from docx import Document
from io import BytesIO
import uuid
from pipeline import (
    parser_sommaire, generate_prompts_from_structure, load_documents_from_paths, group_docs_by_section,
    split_text, chunk_sections_with_context, embed_text, create_collection, upsert_embeddings,
    search_in_qdrant, scaleway_llm, generate_sections_from_prompts,
    delete_collection, generer_docx
)

st.set_page_config(page_title="G√©n√©rateur documents", page_icon="üìÑ")

# V√©rifie si l'utilisateur est connect√©
if not st.session_state.get("authentication_status"):
    st.warning("Veuillez vous connecter dans la page *Mon Compte* pour acc√©der √† cette fonctionnalit√©.")
    st.stop()
    
# Initialisation des variables de session 
if 'document_buffer' not in st.session_state:
    st.session_state.document_buffer = None
if 'generated_document_name' not in st.session_state:
    st.session_state.generated_document_name = "document_ia.docx"

st.title("üìÑ G√©n√©rateur de document IA")
st.markdown("Remplis le formulaire et r√©cup√®re un document structur√© g√©n√©r√© par l'IA.")

# Formulaire de saisie
with st.form("formulaire_doc"):
    titre_doc = st.text_input("Titre du document")
    finalite_doc = st.text_input("Finalit√© du document")
    sommaire = st.text_area("Sommaire", height=150, help="1. Partie A\n2. Partie B")

    uploaded_files = st.file_uploader("Documents sources (PDF)", type=["pdf"], accept_multiple_files=True)

    col1, col2, col3 = st.columns([1, 1, 1])
    with col2:
        submit = st.form_submit_button("G√©n√©rer le document")

# --- TRAITEMENT DIRECT AVEC LA PIPELINE ---
if submit:
    if not titre_doc or not finalite_doc or not sommaire:
        st.warning("Merci de remplir tous les champs.")
    else:
        with st.spinner("üß† Traitement en cours..."):
            try:
                api_key = st.secrets["api_key"]
                llmsherpa_api_url = st.secrets["llmsherpa_api_url"]
                qdrant_url = st.secrets["qdrant_url"]
                model_embed = "bge-multilingual-gemma2"  
                model_llm = "llama-3.3-70b-instruct"  
                
                pdf_paths = []

                for uploaded_file in uploaded_files:
                    if not uploaded_file.name.lower().endswith(".pdf"):
                        st.warning(f"{uploaded_file.name} n‚Äôest pas un PDF, ignor√©.")
                        continue

                    fn = f"{uuid.uuid4().hex}.pdf"
                    rel_path = os.path.join("uploaded_pdfs", fn)   
                    os.makedirs("uploaded_pdfs", exist_ok=True)

                    with open(rel_path, "wb") as f:
                        f.write(uploaded_file.read())

                    pdf_paths.append(rel_path) 
                
                structure = parser_sommaire(sommaire)
                prompts = generate_prompts_from_structure(structure)
                
                collection_name = create_collection(host=qdrant_url, vector_size=3584)  
                
                with st.status("üìÑ Analyse des documents en cours.."):
                    docs = load_documents_from_paths(pdf_paths, llmsherpa_api_url=llmsherpa_api_url)
                    section_map = group_docs_by_section(docs)
                    chunks = chunk_sections_with_context(section_map, 4000)

                    st.write("‚úÖ Texte extrait et d√©coup√©.")

                    embeddings = embed_text(chunks, model=model_embed, api_key=api_key)
                    upsert_embeddings(embeddings, collection_name, host=qdrant_url)
                    
                    st.write("‚úÖ Embeddings g√©n√©r√©s et ins√©r√©s.")
                
                with st.status("R√©daction du document..."):
                    results = generate_sections_from_prompts(
                        prompts=prompts,
                        titre_doc=titre_doc,
                        finalite_doc=finalite_doc,
                        collection_name=collection_name,
                        embedding_fn=embed_text,
                        llm_fn=scaleway_llm,
                        model_embed=model_embed,
                        model_llm=model_llm,
                        api_key=api_key,
                        host=qdrant_url,
                        top_k=3
                    )

                    st.write("‚úÖ R√©daction termin√©e.")
                
                result_json = {
                    "titre_document": titre_doc,
                    "sommaire": sommaire.strip().split('\n'),
                    "sections": results
                }
                
                st.session_state.document_buffer = generer_docx(result_json)
                st.session_state.generated_document_name = f"{titre_doc}.docx"
                
                delete_collection(collection_name, host=qdrant_url)  
                
                for path in pdf_paths:
                    try:
                        os.unlink(path)
                    except:
                        pass
                
                st.success("üìÑ Le document est pr√™t √† √™tre t√©l√©charg√© !")

            except Exception as e:
                st.error(f"Erreur lors du traitement : {str(e)}")
                import traceback
                st.code(traceback.format_exc())

# bouton de t√©l√©chargement 
if st.session_state.document_buffer is not None:
    st.download_button(
        label="üì• T√©l√©charger le document Word",
        data=st.session_state.document_buffer,
        file_name=st.session_state.generated_document_name,
        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        key="docx-download"
    )
